{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import time as timer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "pdf_path = \"APBiology-OP.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store  = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model = \"llama3\", format = \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df['prompt'].tolist()\n",
    "options = df[['A', 'B', 'C', 'D', 'E']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcq(question, choices):\n",
    "    formatted_question = {\"question\": question}\n",
    "    formatted_options= {\"options\": { }}\n",
    "    for i, choice in enumerate(choices):\n",
    "        formatted_options[\"options\"][chr(65+i)] = choice\n",
    "    return formatted_question, formatted_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# Define your desired data structure.\n",
    "class Answer(BaseModel):\n",
    "    # answer: str = Field(description=\"your single letter of option that is the right answer, without any spaces or special characters.\")\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the output parser\n",
    "parser = JsonOutputParser(pydantic_object=Answer)\n",
    "# Define the model behavior and prompt tempalte\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the following multiple choice question:\\n{format_instructions}\\n{question}\\n{options}\\n{context}\",\n",
    "    question=\"Question: {question}\",\n",
    "    options=\"Options:\\n{options}\",\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    input_variables={\"question\", \"options\", \"context\"}\n",
    ")\n",
    "\n",
    "# Define the questions\n",
    "questions = {\n",
    "    \"question\": \"What is the function of the Golgi apparatus?\",\n",
    "}\n",
    "options = {\n",
    "    \"options\": [\"A) Protein synthesis\", \"B) Lipid synthesis\", \"C) Carbohydrate synthesis\", \"D) Protein modification\", \"E) DNA replication\"]\n",
    "}\n",
    "\n",
    "context = \"The Golgi apparatus is an organelle found in most eukaryotic cells. It is made up of membrane-bound sacs called cisternae. The Golgi apparatus is responsible for modifying, sorting, and packaging proteins for secretion. It also plays a role in lipid synthesis and carbohydrate synthesis. The Golgi apparatus is involved in the transport of proteins and other molecules within the cell.\"\n",
    "\n",
    "# Run the chain\n",
    "chain = prompt | llm | parser\n",
    "start = timer.time()\n",
    "results = chain.invoke({\"question\": questions, \"options\": options, \"context\": context})\n",
    "end = timer.time()\n",
    "\n",
    "# Print the results\n",
    "print(results['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(llm, question, options):\n",
    "    # Define the output parser\n",
    "    parser = JsonOutputParser(pydantic_object=Answer)\n",
    "    # Define the model behavior and prompt tempalte\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Answer the following multiple choice question:\\n{format_instructions}\\n{question}\\n{options}. \\n You should give an answer in the form of a single letter, without any spaces or special characters.\",\n",
    "        question=\"Question: {question}\",\n",
    "        options=\"Options:\\n{options}\",\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        input_variables={\"question\", \"options\"}      \n",
    "    )\n",
    "    # Run the chain\n",
    "    chain = prompt | llm | parser\n",
    "    start = timer.time()\n",
    "    results = chain.invoke({\"question\": question, \"options\": options})\n",
    "    while \"answer\" not in results:\n",
    "        results = chain.invoke({\"question\": question, \"options\": options})\n",
    "    end = timer.time()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    if index >= 100: \n",
    "        break\n",
    "    formatted_question, formatted_options = format_mcq(row['prompt'], [row['A'], row['B'], row['C'], row['D'], row['E']])\n",
    "    print(formatted_question, formatted_options)\n",
    "    answer = ask_model(llm, formatted_question, formatted_options)\n",
    "    # results.append(answer)\n",
    "    print(\"answer:\",answer)\n",
    "    print()\n",
    "\n",
    "# df['Model Answer'] = results\n",
    "# print(df[['prompt', 'Model Answer']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the function of the Golgi apparatus?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Query: \", query)\n",
    "start = timer.time()\n",
    "answer = chain({\"query\": query})\n",
    "end = timer.time()\n",
    "print(\"Answer: \", answer)\n",
    "print(\"Time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
