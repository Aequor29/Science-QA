{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required packages\n",
    "You also need to install [Ollama](https://ollama.com/) to run our project. After installing, enter Ollama run llama3 in the terminal to start the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install pypdf\n",
    "%pip install gpt4all\n",
    "%pip install chromadb\n",
    "%pip install pandas\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "import time as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the textbook data and split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the PDFs\n",
    "pdf_paths = [\n",
    "    \"APBiology-OP.pdf\",\n",
    "    \"Theodore E. Brown et al. - Chemistry_ The Central Science-Pearson (2017).pdf\",\n",
    "    \"David Halliday, Robert Resnick, Jearl Walker - Fundamentals of Physics Extended-Wiley (2013).pdf\"\n",
    "]\n",
    "\n",
    "# Initialize a list to hold all text chunks from all books\n",
    "all_documents = []\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    # Load the data from each PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    data = loader.load()\n",
    "    \n",
    "    # Split the data into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    all_splits = text_splitter.split_documents(data)\n",
    "    # Collect all chunks in a single list\n",
    "    if len(all_documents) == 0:\n",
    "        all_documents = all_splits\n",
    "        vector_store = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n",
    "    else:\n",
    "        all_documents += all_splits\n",
    "        vector_store.add_documents(all_splits)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model = \"llama3\", format = \"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "questions = df['prompt'].tolist()\n",
    "options = df[['A', 'B', 'C', 'D', 'E']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcq(question, choices):\n",
    "    \"\"\"\n",
    "    Formats a multiple-choice question and its corresponding choices into separate dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        question (str): The text of the multiple-choice question.\n",
    "        choices (list): A list of strings where each string is a potential answer choice.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries:\n",
    "            - The first dictionary with the key 'question' pointing to the question text.\n",
    "            - The second dictionary with the key 'options' containing a dictionary of options,\n",
    "              where each key is a letter ('A', 'B', 'C', etc.) corresponding to the choice's index in the input list.\n",
    "\n",
    "    \"\"\"\n",
    "    formatted_question = {\"question\": question}\n",
    "    formatted_options= {\"options\": { }}\n",
    "    for i, choice in enumerate(choices):\n",
    "        formatted_options[\"options\"][chr(65+i)] = choice\n",
    "    return formatted_question, formatted_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define desired answer structure.\n",
    "We require the model output a single letter since it is easy to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseModel):\n",
    "    answer: str = Field(description=\"your single captial letter of option that is the right answer, without any spaces or special characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "We provide two different type of models. One with the textbooks embedding and one without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model_with_out_embeddings(llm, question, options):\n",
    "    \"\"\"\n",
    "    Invokes a language model to answer a multiple-choice question without using embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        llm (LanguageModel): The language model to use for answering the question.\n",
    "        question (str): The text of the multiple-choice question.\n",
    "        options (list): A list of strings representing the multiple-choice options.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the validated answer from the language model. The key 'answer'\n",
    "              will hold the value of the model's response, expected to be a single letter.\n",
    "    \"\"\"\n",
    "    parser = JsonOutputParser(pydantic_object=Answer)\n",
    "    # Define the model behavior and prompt tempalte(no context)\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Answer the following multiple choice question:\\n{format_instructions}\\n{question}\\n{options}. \\n You should give an answer in the form of a single letter, without any spaces or special characters.\",\n",
    "        question=\"Question: {question}\",\n",
    "        options=\"Options:\\n{options}\",\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        input_variables={\"question\", \"options\"}      \n",
    "    )\n",
    "    # Run the chain\n",
    "    chain = prompt | llm | parser\n",
    "    start = timer.time()\n",
    "    results = chain.invoke({\"question\": question, \"options\": options})\n",
    "    while \"answer\" not in results:\n",
    "        results = chain.invoke({\"question\": question, \"options\": options})\n",
    "    end = timer.time()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model_with_embeddings(llm, question, options, retriever):\n",
    "    \"\"\"\n",
    "    Invokes a language model to answer a multiple-choice question with embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        llm (LanguageModel): The language model to use for answering the question.\n",
    "        question (str): The text of the multiple-choice question.\n",
    "        options (list): A list of strings representing the multiple-choice options.\n",
    "        retriever (Retriever): The retriever object used to retrieve the context for the question.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the validated answer from the language model. The key 'answer'\n",
    "              will hold the value of the model's response, expected to be a single letter.\n",
    "    \"\"\"\n",
    "    parser = JsonOutputParser(pydantic_object=Answer)\n",
    "    # Define the model behavior and prompt tempalte\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Answer the following multiple choice question:\\n{format_instructions} \\n{context} \\n{question}\\n{options}. \\n \",\n",
    "        question=\"Question: {question}\",\n",
    "        options=\"Options:\\n{options}\",\n",
    "        context=\"Context: {context}\",\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        input_variables={\"question\", \"options\", \"context\"}      \n",
    "    )\n",
    "    # Run the chain\n",
    "    chain = prompt | llm | parser\n",
    "    start = timer.time()\n",
    "    docs = retriever.invoke(question[\"question\"])\n",
    "    results = chain.invoke({\"question\": question, \"options\": options, \"context\":docs[0].page_content})\n",
    "    while \"answer\" not in results:\n",
    "        results = chain.invoke({\"question\": question, \"options\": options, \"context\":docs[0].page_content})\n",
    "    end = timer.time()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With embeddings\n",
    "results_embeded = []\n",
    "for index, row in df.iterrows():\n",
    "    print(index)\n",
    "    formatted_question, formatted_options = format_mcq(row['prompt'], [row['A'], row['B'], row['C'], row['D'], row['E']])\n",
    "    answer = ask_model_with_embeddings(llm, formatted_question, formatted_options, retriever)\n",
    "    results_embeded.append(answer[\"answer\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without embeddings\n",
    "results_no_embeded = []\n",
    "for index, row in df.iterrows():\n",
    "    print(index)\n",
    "    formatted_question, formatted_options = format_mcq(row['prompt'], [row['A'], row['B'], row['C'], row['D'], row['E']])\n",
    "    answer = ask_model_with_out_embeddings(llm, formatted_question, formatted_options)\n",
    "    print(answer)\n",
    "    results_no_embeded.append(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results into npy arrays\n",
    "np.save(\"results_embeded.npy\", results_embeded)\n",
    "np.save(\"results_no_embeded.npy\", results_no_embeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the accuracy\n",
    "The following code only calculate accuracy for a single run of the experiment. We recommend to run the model for several time and take the average accuracy to get more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = df['answer'].tolist()\n",
    "embedded_accuracy = sum([1 for i in range(len(answers)) if answers[i] == results_embeded[i]]) / len(answers)\n",
    "print(f\"Embedded Accuracy: {embedded_accuracy}\")\n",
    "no_embedded_accuracy = sum([1 for i in range(len(answers)) if answers[i] == results_no_embeded[i]]) / len(answers)\n",
    "print(f\"Without Embedded Accuracy: {no_embedded_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
